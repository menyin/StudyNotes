？哪里有免费测试服务器
   https://baijiahao.baidu.com/s?id=1752442790055696298&wfr=spider&for=pc

？让如何让自己的电脑被公网上的其他电脑访问到？
    除了内网穿透和ddns之外还有其他办法吗？
    https://www.zhihu.com/question/543455641/answer/2576249694

*DDNS（Dynamic Domain Name Server，动态域名服务）是将用户的动态IP地址映射到一个固定的域名解析服务上，用户每次连接网络的时候客户端程序就会通过信息传递把该主机的动态IP地址传送给位于服务商主机上的服务器程序，服务器程序负责提供DNS服务并实现动态域名解析。
 ？那么家庭宽带的动态公网ip就能通过DDNS实现让外网访问自己的服务器
   详见 csdn 《配置项目外网访问（公网IP+DDNS）》
 阿里云：
 用户登录名称 geduo@1640204298257495.onaliyun.com
 AccessKey ID LTAI5tNbn5zEpe76zu72qLF8
 AccessKey Secret ocGd5TGylWQpBZ9crocnfa0iPk36ec
 腾讯云：
主账号ID 100008721461 
 用户名 geduo 
 登录密码 - 
 SecretId AKID3zteDZhcomVxCF9pIZfXCrKYVFn8GCyZ 
 SecretKey UoxwiHOJ2XvCKG0WpwoStBML6ez61iHg

？局域网在物理上是不是一定要在同一区域或者连接相同的路由器

？家庭宽带申请的公网ip和企业宽带公网ip有什么区别
  https://m.163.com/dy/article/HF54ASC10538U2BB.html

？多云主机搭建局域网，详见csdn《不同云厂商云主机实现内网互通解决方案》
   其中有一步是设置自己的子域名，已经设置为geduo.xxxxx

*梯子：
 一元机场 https://xn--4gq62f52gdss.com/#/register?code=NaPksl5S  账号：845257580@qq.com
 Omega https://mymonocloud.com/   注册账号：845257580@qq.com，登录账号fanyinyumeng
 chatGPT用一元机场被限速了，因为一元机场切换节点其实是假切换，因为IP都是一样的。



？家庭宽带能申请固定ip地址吗？
   一般只能申请动态公网ip

*多台Linux服务器之间可以配置免密码互相SSH登陆

*一般开发者不会以root用户登录或启动（确保安全，养成良好的习惯）

*抓包工具相关设置 详见csdn《工具抓包Charles配置HTTPS步骤》
 学习bili《软件测试教程Charles抓包工具测试实战》 
 能做什么：支持http/https代理、流量控制、接口并发请求、重发网络请求、断点调试
 对比fiddler：支持linux/MacOs、按域名/接口查看报文、反向代理、选择网络类型、解析AMF协议
 注意：如果charles客户端在本地电脑，则本地电脑浏览器默认会进行代理访问，无须再设置。 如果是手机等其它终端则需要设置代理


*黑马推荐大数据学习时，可用亚马逊aws免费云服务器
 详见http://sh.itheima.com/areanew/shenzhen/cloud/20190801/192005.html

*云服务器系统学习详见黑马《2023新版Hadoop课程》学习资料“0-前置章节-环境准备.pptx”

？网段，局域网
   云服务器中如何设置虚拟的局域网（即VPC（virtual private cloud）私有虚拟局域网）,VPC就相当于拉了一个家庭带宽，属于个人的私有网络

*在云服务创建VPC时，会让填写网段，如192.168.0.0/16 其中16代表可提供/16级别的子网掩码（65535个IP）,即2的16次方
 注：192.168.0.0相当于后两位的0.0是可变的，他们会可以有2^8*2^8个值作为ip，即2^16
       通过VPC创建，每个云服务器实例创建时可以分配一个私网地址（即局域网IP）
       VPC内都共享公网IP和带宽
       在云服务器开放端口时，有一列“授权对象” 即允许哪些IP访问这个开放端口，如0.0.0.0即所有



************************************** k8s beign *************************************************
*一些k8s环境安装所需镜像地址：
 registry.aliyuncs.com/google_containers/nginx-ingress-controller:0.30.0
 registry. aliyuncs. com/doogie_containers/kude-proxy
 registry.aliyuncs.com/google containers/kube-apiserver registry.aliyuncs.com google_containers kube-scheduler
 registry.aliyuncs.com/google_containers/kube-controller-manager registry.aliyuncs.com/google containers/coredns registry.aliyuncs.com google_containers etcd

*kubectl常用命令：
 kubectl config view查看k8s集群配置
 kubectl logs podID -n nsName 查看pod运行日志，用于查找定位问题
 kubectl get ns 查看所有名称空间
 kubectl get pod -n nsName 查看指定名称空间下的pod
 kubectl get all -n nsName 查看指定名称空间下的所有资源
 kubectl describe resou 查看指定资源的状态信息

Pod调度
​ 在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式：

k8s集群初始化时得到的信息：

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.1.200:6443 --token nq38zt.hmtzx4m4zl49blll \
    --discovery-token-ca-cert-hash sha256:c35c53b6acffbddf28fa7d3b53f7043c352b100d2dff6e783bf0d2642cb83b52

*注意以上添加集群节点的命令的token如果失效，则需要在master节点重新生成一个替换即可（重新生成token命令：kubeadm token create）

************************************** k8s end *************************************************
***********具体详情资料详见网盘，黑马课程《容器集群管理系统k8s从入门到精通》**********
***********深入学习，见《完整版Kubernetes（K8S）全套入门+微服务实战项目》**********
*k8s可以看做是一个资源调度操作的服务。在多台服务器中安装，然后进行统一管理。 
 这些服务器分为master节点和work节点，master负责调度，work负责实际资源的使用。 

*windows也可以安装kubectl.exe来连接管理k8s集群，详见：
https://kubernetes.io/zh-cn/docs/tasks/tools/install-kubectl-windows/
https://www.syrr.cn/news/66775.html?action=onClick

*deploment、replicaset、statefullset、service、ingress这几个是重点，也是经常用到的
 开发人员更多的是用各种控制器

*helm是k8s的包管理工具，类似java的maven，前端的npm等
 helm三个核心概念：
     chart：它包含了创建一个应用所需要的deploment、service等资源信息。是一种资源
     config：chart创建相关的配置信息
     release：相当于chart的一个实例
 注意：helm命令要求kubectl能用，rancher的RKE创建的k8s集群的主机用kubectl时会出问题，可在rancher上找kubeconfig文件内容复制到~/.kube/config文件里即可（（注意添加权限 chmod 600 /root/.kube/config））
 仓库添加：有些教程的阿里云等仓库地址有误，可参考csdn《国内helm快速安装和添加常用charts仓库》
 常用命令：
     helm repo list 查询已添加的仓库
     helm repo add bitnami https://charts.bitnami.com/bitnami    为helm添加仓库 
     helm  search repo redis 查询应用仓库（此处是redis）
     helm search repo bitnami/kafka --versions  可以查看chart版本和kafka版本号对应关系
     helm show chart bitnami/kafka --version 3.4.1  可以查看kafka3.4.1版本对应chart版本
     helm show values test-repo/tomcat 查询应用chart的使用手册
 特别注意：在rancher中RancherUI的【catalog应用商店】 对应 ClusterExplorer仪表盘的【Apps & Marketplace应用市场】（Apps & Marketplace在左上角比较隐秘），很别扭，
                而且两者的已添加chart仓库（或者应用商店）的列表居然不一致，导致查找kafka应用时，搜出的结果不一样。  
                在应用商店搜索出来kafka应用bitnami/schema-registry和bitnami/kafka  前者可以正常安装，但是后者却怎么也安装不了
 特别注意：要升级在Rancher的应用商店里已安装的APP时，只能点击升级按钮，并不能添加一些升级命令携带的参数，如升级redis时，需传递密码参数，但是却无法添加，所以就会报错。
                helm upgrade --history-max=5 --install=true --namespace=redisc --timeout=10m0s --values=/home/shell/helm/values-redis-cluster-8.6.2.yaml --version=8.6.2 --wait=true redisc0 /home/shell/helm/redis-cluster-8.6.2.tgz 
                解决方案：helm命令行参数可以通过--set password=sdew  进行设置，也可以再values.yaml进行配置。两者可能最后都是变成命令行参数，而--set优先级更高

*redis集群搭建：
   bitnamicharts/redis  只可一主多从，从节点可配置
   bitnamicharts/redis-cluster 可多主多从，都可配置。 主要要看readme.md文档。 发现要配置password、cluster.update.addNodes、cluster.update.currentNumberOfNodes、cluster.nodes（似乎还要设置cluster.init=false）
   实践心得：
        *rancher界面反馈的一些资源的状态并不是很可靠，比如应用市场扩容redis集群时，已经从6节点扩容到8节点，但是rancher已安装APP里查看redis集群还是6。到pods资源界面发现都成功了。 用kubectl查看也是8个正常状态
         因此当rancher报错或者状态不正常时，用kubectl describe|logs等命令再次确认好，因为rancher反馈的资源状态会比较慢或者不全。
        *rancher新旧管理界面分别有应用商店和应用市场两个类似但不完全一致的概念，初始状态下两个模块里的应用仓库列表不一样。 两者操作界面不一样，一般使用新界面的应用市场安装应用。
        *rancher应用市场装应用实际也是执行helm命令，而没法传递命令行参数（用--set ），实际上values.yaml的配置最后也是变成helm执行的参数运行，所以可在values.yaml里直接加。但不同应用参数需要看readme.md等文档
        *rancher或helm里查看应用是不是同一个应用，第一看应用所在仓库，第二看应用chart版本
 注意：
     *helm查找到的chart包只是开发者们整理的一系列yaml配置，以便我们按模板启动k8s容器。相同应用可能有多个chart，具体用哪个仓库的chart就要去了解。 
      如kafka有三个chart：wurstmeister/kafka、confluentinc/cp-kafka 和 bitnami/kafka。 其中confluentinc/cp-kafka是比较完备的方案
     *在rancher中添加应用商店类似于helm添加仓库，但应该说它比helm仓库要多一层“包装”。 如要查找bitnami/kafka，则需要添加应用商店，URL为https://charts.bitnami.com/bitnam
      如果上述URL添加出问题或启动应用出问题，可以换https://github.com/bitnami/charts.git，并且分支改成main而不是master，具体到github对应仓库查看版本（应该是要用github仓库才行）


？查找redis的chart有两个，应该用哪个？bitnami/redis-cluster和bitnami/redis有什么区别？
   bitnami/redis-cluster是集群，支持分片，高可用等
   bitnami/redis只是单机，可以设置从节点做冗余备份，即主从节点。

*Rancher UI中的应用商店和Cluster Explorer中的Chart仓库是两个不同的概念，它们提供了不同的功能和用途。
 *应用商店（App Store）：
   Rancher UI中的应用商店提供了一种简化的方式来浏览、部署和管理预定义的应用程序。应用商店通常包含经过验证和打包的应用程序图表，可以轻松地通过界面进行部署。您可以在应用商店中选择所需的应用程序，配置相关参数，并将其部署到您的集群中。Rancher提供了一些默认的应用商店，您还可以根据需要添加自定义的应用商店。
 *Chart仓库（Chart Repository）：
  Cluster Explorer中的Chart仓库是基于Helm的概念，用于管理和共享Helm Chart。Chart仓库是存储Helm Chart的位置，它可以是本地或远程的存储库。您可以添加多个Chart仓库，然后通过Cluster Explorer浏览、搜索和安装可用的  Chart。Chart仓库中的Chart通常由社区或第三方提供，您可以通过Chart仓库获得更多的Chart选项。
 *总结来说，应用商店提供了一种更加集成和简化的方式来浏览和部署预定义的应用程序，而Chart仓库则更加注重于Helm Chart的管理和共享，使您能够访问更广泛的Chart选项。您可以根据具体需求选择使用应用商店或Chart仓库来部署和管理应用程序。

*为什么会出现以下报错的情况，RuncherUI里的Catalog和Cluster Explorer里的Apps & Marketplace不是一样的概念吗？
  A chart repository is a Helm repository or Rancher's git based application catalog. It provides the list of available charts in the cluster.
  因为Catalog 和 Apps & Marketplace 在 Rancher 中是不同的概念：
  Catalog：Catalog 是 Rancher 的内置功能，它是基于 Git 的应用程序目录，用于存储和管理应用程序模板（charts）。Catalog 中包含了一系列预定义的应用程序模板，可以通过 Rancher UI 或命令行来使用。用户可以从 Catalog 中选择适用的应用程序模板进行部署。
  Apps & Marketplace：Apps & Marketplace 是 Rancher Cluster Explorer 中的功能。它是基于 Rancher 的 Chart 仓库，提供了可用 charts 的列表。用户可以在 Apps & Marketplace 中查找并安装不同的 charts，包括 Helm charts 和 Rancher App templates。这些 charts 可以来自于 Rancher 的内置 Chart 仓库，也可以是用户自定义的 Chart 仓库。
  所以，虽然两者都提供了应用程序模板（charts）的功能，但是 Catalog 是 Rancher 特定的功能，而 Apps & Marketplace 则是 Cluster Explorer 中的一个界面，可以展示不同的 Chart 仓库中的 charts。
  个人感觉建议使用Apps & Marketplace 



？注意不管是Catalog或 安装集群类的应用，有时候会安装不起来 pending状态，报以下类似错误。大多数是PVC和PV的问题
  StatefulSet is not ready: redisc/redisc-redis-cluster. 0 out of 6 expected pods are ready


Regenerate response








*confluentinc/cp-kafka大致原理详见腾讯云开发者社区《Schema Registry在Kafka中的实践》 https://cloud.tencent.com/developer/article/2192136
 大概就是增加了消息体的schema规约注册维护

*每个k8s节点主机都会包含kubelet，kube-proxy，docker三个组件
 所有操作都会通过apiserver（相当于入口，resful接口），如命令行或rancher都是调用apiserver

*推荐安装方式二进制方式，其次命令行安装



*k8s资源管理方式：
   命令式对象管理：直接使用命令去操作kubernetes资源
   kubectl run nginx-pod --image=nginx:1.17.1 --port=80
   命令式对象配置：通过命令配置和配置文件去操作kubernetes资源
   kubectl create/patch -f nginx-pod.yaml
   声明式对象配置：通过apply命令和配置文件去操作kubernetes资源
   kubectl apply -f nginx-pod.yaml

？用kubectl run pod --image=nginx -n dev创建了一个pod
   然后用kubectl get pod -n dev查看时，STATUS状态为creating或ImagePullBackOff
  magePullBackOff错误比较简单，镜像下载失败，要么网络设置有问题，要么没有设置镜像源，详见csdn《【k8s】ImagePullBackOff 错误处理》
  目前镜像无法下载
  ！发现node2节点ssh都无法连接（在192.168.1.30电脑里可以连接），后面复制了node1虚拟机为node2重新配置了ip为192.168.1.222。具体原因应该是192.168.1.202被冲突了
    ？那么如何删除集群节点，重新添加：
     #重置kubeadm用命令：kubeadm reset
     #删除k8s配置文件和证书文件用命令：rm -rf /etc/kubernetes/kubelet.conf /etc/kubernetes/pki/ca.crt #删除k8s配置文件和证书文件
     #使用master初始化k8s集群时生成的带token的命令进行节点添加（要用node2的root账号）：
         kubeadm join 192.168.1.200:6443 --token nq38zt.hmtzx4m4zl49blll \
         --discovery-token-ca-cert-hash sha256:c35c53b6acffbddf28fa7d3b53f7043c352b100d2dff6e783bf0d2642cb83b52
        注意：以上token可以通过kubeadm token list查看，如果没有token可以通过kubeadm token create创建
                   以上命令再加上--control-plane参数可以将节点添加为master节点
   ？那么如何删除掉之前部署运行的nginx容器呢（因为master、node1都有部署成功，node2由于ip网络原因未成功）
      删除nginx的pod、service等，重新部署运行后，用kubectl get pods,service查看pod/nginx的状态，发现一直是ContainerCreating //等后面再想办法解决。
     ！运行kubectl get pod -n kube-system查看k8s集群相关组件，发现flannel网络插件的状态是Init:ImagePullBackOff，说明没有安装成功，难怪上面的nginx部署不了（处于ContainerCreating状态）
     ！针对flannel网络插件无法拉取安装都是建议将kube-flannel.yml替换成quay-mirror.qiniu.com，尝试都不行。 后面搜索“安装kube-flannel时怎么使用阿里镜像源”找到csdn《kube-flannel.yml镜像我已经修改成阿里云的了》可
用。注意：如果kubectl get nodes 节点状态还是NotReady，可以参考csdn《K8S集群NotReady问题处理》  解决
    
   ？当我kubectl run pod --image=nginx -n dev创建运行一个nginx，然后kubectl delete pod pod-864f9875b9-pcw7x删除不了，要加名称空间：kubectl delete pod pod-864f9875b9-pcw7x -n dev
      但是当我删除了nginx的pod，它又会自动创建（通过kubectl get pod -n dev可查看）！这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建。解决方案就是对应的控制器也要删除：
      # 先来查询一下当前namespace下的Pod控制器：kubectl get deploy -n  dev
      # 接下来，删除此Pod的Pod控制器  kubectl delete deploy nginx -n dev
      注意：Deployment是一种控制器
      

*虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：
 - Pod IP 会随着Pod的重建产生变化
 - Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问
 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。

*可通过一个命令来查看每种资源的可配置项
 #   kubectl explain 资源类型         查看某种资源可以配置的一级属性
 #   kubectl explain 资源类型.属性     查看属性的子属性

*进入容器命令（pod-configmap是pod资源名称）：[root@master ~]# kubectl exec -it pod-configmap -n dev /bin/sh

*查看k8s所有资源信息命令：kubectl api-resources

*kubectl常用的一些flage参数 
 -o 即--output 参数自定义输出格式，如：yaml
 -f  即--filename，就是文件名称
 -w 即--watch实时检测操作对象的更新  如命令：kubectl get deployment -n cny -w


*run命令通常用于创建运行一个deploy资源，如： kubectl run nginx --image=nginx:1.17.1 --port=80 --

*常用命令：
 - apiVersion   \<string>     版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到
 - kind \<string>                类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到

*常用配置解释：
  spec.hostNetwork 是否使用主机网络。即如：pod在安装在192.168.1.201节点上，设置hostNetwork=true,在该pod可以用到该201ip地址，并且要占用201节点一个端口。这样201端口容易不够或冲突
  spec.containers.image: nginx:1.17.1 注意这里单写了应用镜像名+版本好，则它在搭建k8s集群时安装docker时指定的镜像源中pull镜像
  spec.containers.imagePullPolicy：Always|IfNotPresent|Never   这里说的本地镜像应该是相对于pod所在的宿主机上的本地docker镜像
  kubectl get pods -n dev执行后READY列值为1/2表示，当前pod有两个容器（根容器不算），有1个已经准备就绪

*-f的含义：在Kubernetes中，kubectl create -f命令用于从YAML文件创建资源对象。其中，-f是一个缩写，表示--filename，其作用是指定要创建的Kubernetes资源对象的YAML文件路径。
  例如，kubectl create -f pod-resources.yaml将会创建一个包含在pod-resources.yaml文件中定义的Pod资源对象。
  如果您需要创建多个资源对象，可以将多个YAML文件路径作为参数传递给kubectl create -f命令，例如：kubectl create -f pod1.yaml -f pod2.yaml

以下命令结果的CPU(cores)值为2m是什么意思？！2m代表2毫核。在 Kubernetes 中，CPU 的单位是 "millicores"（毫核），表示 1/1000 个 CPU 核心。在此例中，该 Pod 被限制为最大占用 2 毫核的 CPU 资源。
[root@cdh-6 1.8+]# kubectl top pod -n database
NAME                    CPU(cores)   MEMORY(bytes)
redis-fb6bfcfdd-28qm2   2m           10Mi

**特别注意在运行kubectl 相关的命令，尽量加上名称空间参数，否则可能会报错资源找不到。因为如果不指定-n名称空间，则会在默认名称空间里查找操作资源

*作为测试环境

*Pod生命周期
  *用一个yaml文件创建多个容器，会按定义顺序一个个创建，成功一个再创建一个。
  *pod的创建和终止过程需特别关注理解，详见黑马文档2天“pod的创建过程”文本处拓扑图
  *Pod会出现5种**状态**：
    pedding（apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像）
    running（pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成）
    succeded（pod中的所有容器都已经成功终止并且不会被重启）
    failed（所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态）
    unknown（未知）
*亲和性调度
 原理：主要是根据node或pod进行亲和性判断，然后判断亲和条件，然后再根据是“硬性”或“软性”匹配，进行调度。
          也就是说，先设置是node或pod亲和性，然后设置亲和条件，然后如果是“硬性”则如果条件不满足就调度不成功，如果是“软性”则条件不满足就调度到其它node或其它pod

*Node污点：通过在node节点资源上设置污点，然后设置不同策略，实现排斥pod调度进来的功能。 具体策略如下
    - PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度
    - NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod
    - NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离

*pod容忍：虽然node节点设置了污点可以拒绝pod进来，但是pod也可以设置容忍来忽略node上的污点，“强行”调度进入有污点的node


*Pod控制器
 Deployment：简称Deploy，当pod的资源出现故障时，会尝试进行重启或重建pod。

*几种常用Pod控制器的区别

*deployment也是pod控制器，支持两种pod更新策略:`重建更新`和`滚动更新`。滚动更新杀一小部分就启动一部分，更新过程可能存在2个版本pod

*服务分类：
      有状态：会对本地环境产生依赖性（如：会存储数据到本地的redis应用，重启一个redis时它会从本地的持久化数据进行恢复数据）。如果是无状态服务会重建pod，则重建后的pod会面临IP已变、存储丢失、顺序错乱三个问题。
                 IP已变：重建的IP变化了，则意味着要访问它就不能用它的IP，而是要用服务名等其它方式。 实践中一般用域名
                 存储丢失：如一个集群的主pod数据挂载到一个固定地方，那么主pod重启后怎么保证他依然挂载到之前相同的地方。PVC指定SC，SC动态创建PV并与PVC关联绑定。
                 顺序错乱：对于访问或者数据存储，pod是有顺序性的，如redis集群有主从之分，pod1代表主节点，pod2，pod3代表从节点。那么就要保证这个顺序性，让pod重启后还是映射到相同域名和数据存储卷
      无状态：不会对本地环境产生依赖性（如：nginx集群、普通java微服务应用）。
                  
*StatefulSet是一种有状态的服务，通常用于部署有主从关系的应用集群（如mysql集群），理解详见csdn《k8s之StatefulSet详解》、blogs《k8s-StatefulSet控制器-十四》、（先看简书《k8s部署zookeeper集群》再看csdn《k8s部署kafka集群》，也可直接看腾讯云《k8s部署kafka集群「建议收藏」》https://cloud.tencent.com/developer/article/2091950 ）
 大致原理：有状态的服务即所有的pod是有序的，其存储也是有序的。  通过域名名称保证pod重启后内部ip改变后仍然是有序的，并且固定域名名称的存储对应到对应的固定有序的pv和pvc上。
 pod的固定域名格式：<pod-name>.<svc-name>.<namespace>.svc.cluster.local  注意这只能是内网访问。 
 pod的固定名称格式：statefulset创建出来的pod 名称以<StatefulSet name>-<order>开始编号。
 volumeClaimTemplates用于动态生成pvc，pvc名称格式：<volumeClaimTemplates.name>-<pod_name>
        请注意：PVC一定会自动创建，而创建 PV 是否自动创建取决于集群中是否有可用的动态存储类。如果没有动态存储类（即storageClasss），您需要手动创建 PV，并将其与 PVC 关联，以使其可用于 StatefulSet 中的 Pod。
                    storageClasss要绑定一个制备器Provisioner，它就是一个PV的制造机，不同的PV有不同的制备器Provisioner。在Rancher ClusterExplorer中可以查找nfs的制备器来安装。
                   注意逻辑：StatefulSet动态生成PVC需要SC，SC需要动态制备PV的制备器Provisioner，自定义的NFS制备器Provisioner以Deploment形式启动，启动时要求先在NFS所在主机上安装启动NFS服务并配置挂载目录权限（详见黑马k8文档）
 ？为什么要用无头服务(即clusterIP=none)。！因为StatefulSet的pod要通过固定的域名访问以保证每个pod的名称固定，而实际pod内部的ip地址是可变的，如果用ip来访问则保证不了pod是有序有状态的
 ？那么为什么ip不能固定呢。！因为pod被调度到哪个主机节点其实是不确定的，那么ip就也不能固定了。
 ？kafka集群搭建时需要指定zk集群（如：zookeeper.connect=node1.itcast.cn:2181,node2.itcast.cn:2181,node3.itcast.cn:2181），但是zk集群如果伸缩，那么zookeeper.connect怎么跟着动态变化。
 Strimzi  https://blog.csdn.net/weixin_43632687/article/details/128330629
 ？类似zk集群，如果派发到同一主机节点，那么zk暴露的端口号会不会冲突


*PodDisruptionBudget（PDB）：PDB的工作原理是根据标签选择器选择要受其控制的Pod副本集，并根据指定的最小可用副本数来限制可以同时终止的Pod数量。如果终止Pod的操作将导致可用副本数低于PDB中指定的最小值，Kubernetes会拒绝终止操作，以确保足够的副本保持运行。 相当于一种保护措施，当同时终止的Pod数据少于或大于指定值，则k8s会拒绝报错。

*上述HeadLiness的service形成的<pod-name>.<svc-name>.<namespace>.svc.cluster.local 格式域名适用于内网访问。
 而如果要外网访问（ 实际经过测试，压根就不行。），结合ingress构造的域名则访问URL如下（其中example.com是ingress资源yaml配置的host，而pod0为HeadLiness类型service对应的多个pod的其中之一）：
 http://example.com/pod0  
 http://example.com/pod0.my-service.my-namespace.svc.cluster.local  
 ？？场景：k8s集群在192.168.8.xx，本地电脑在192.168.1.27。k8s集群上用helm部署了redis集群（有状态的，3主3从）
       问题：本地电脑上的代码怎么连接到redis集群？  jedis客户端连接其中一台如果产生重定向，会返回目标redis节点pod的内部地址而不是外部地址，这时候jedis再拿这个内部地址是请求不到redis节点pod的
       思路：看用Loadbalance类型的service是否能解决这个需求。

*有个k8s集群和java应用属于同一个局域网，但是Java应用没有部署在k8s集群里面。那么Java应用能通过ClusterIP类型的Service暴露的内部ip访问到k8s里面的pod吗？!是可以的
 同上述场景，那么Java应用能通过HeadLiness的service形成的<pod-name>.<svc-name>.<namespace>.svc.cluster.local 格式域名访问到k8s集群里的pod吗？!不可以
 ********* 最后解决方案：k8s本地联调工具KtConnect   详见简书《k8s本地联调工具kt-connect》或 https://alibaba.github.io/kt-connect/#/zh-cn/guide/quickstart *********

 ？如果出现以上报错的话，有可能是kt-connect路由BUG，可能本地电脑的路由与新加的通往API Server的路由有冲突，增加参数--excludeIps 10.0.8.101/32即可，如果网段冲突比较多，可以扩大网段范围，例如--excludeIps    10.0.8.0/24 参考 issue-302 。
    Failed to setup port forward local:28344 -> pod kt-connect-shadow-gseak:53 error="error upgrading connection: error sending request: Post " https://10.0.8.101:8443/api/v1/namespaces/feature-N/pods/kt-connect-shadow-gseak/portforward ": dial tcp 10.0.8.101:8443: connectex: A socket operation was attempted to an unreachable host."，


作者：欢醉
链接：https://www.jianshu.com/p/82c84289fb6d
来源：简书
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

redis2-redis-cluster-0.redis2-redis-cluster-headless.redisc.svc.cluster.local
tomcat-deployment-7db86c59b7-vbc7s.tomcat-service.test.svc.cluster.local
tomcat-deployment-7db86c59b7-vbc7s.tomcat-service.test.svc.cluster.local

*NodePort类型Service资源中，spec.ports.port、spec.ports.nodePort两者似乎没什么区别？
  spec.ports.port：它是 Service 对外公开的端口号，用于接收来自其他服务或外部客户端的流量。这个端口是 Service 在集群内部使用的端口，用于提供服务，并在 Service 的规范中定义。
  spec.ports.nodePort：它是将 Service 的端口映射到集群节点上的端口。当 Service 类型为 NodePort 或 LoadBalancer 时，Kubernetes 集群中每个节点都会监听指定的 nodePort。它提供了一种在集群外部访问 Service 的方式。
 所以，它们的区别在于：
 port 是 Service 内部使用的端口，用于 Service 和后端 Pod 之间的通信。
 nodePort 是用于从集群外部访问 Service 的端口，将外部流量转发到 Service 的端口上。
 总的来说，spec.ports.port 是在集群内部使用的端口，而 spec.ports.nodePort 是在集群外部访问的端口。



*注意yml中的spec属性的意思是期望资源的状态，也就是最终状态可能不一致。  

*在k8s的ConfigMap创建配置中，"|+" 是 YAML 语法中的一个特殊标记，表示接下来的文本块将保留原始缩进和换行符。在这种情况下，它用于保留 redis.conf 配置文件中的原始格式。
  data.redis.conf: |+ 表示 redis.conf 的值将是一个多行字符串，并且会保留字符串中的缩进。这样做是为了确保在生成的 ConfigMap 中，redis.conf 的内容与原始文件中的内容保持一致。
  在这个例子中，redis.conf 的值是 requirepass 123456，并且保留了该行的缩进（通常为两个空格或四个空格）。

*挂载
 在配置pod的挂载目录时，无论是HostPath或NFS或PV等，注意一下配置的含义：
  volumeMounts:
     - name: mysql-persistent-storage  
       mountPath: /var/lib/mysql

  template: 
    spec:                          
      containers:              
          volumeMounts:
            - name: mysql-persistent-storage #指的是对应的实际存储对应，如此处使用HostPath类型存储，则mysql-persistent-storage就是对应volumes.name:mysql-persistent-storage配置的mysql-pv-claim
              mountPath: /var/lib/mysql  #指的是pod的容器中实际的数据存储目录
      volumes:
        - name: mysql-persistent-storage
          persistentVolumeClaim:
            claimName: mysql-pv-claim

*仪表盘
 kube-dashboard、Rancher、

*数据存储：
  *企业级应用一般用NFS类型和高级存储类型
  *PV和PVC：PV是屏蔽了具体存储类型（如NFS类型）的配置，做统一化，它是集群级别资源（没有namespace归属）。  而PVC是有namespace归属，它是对PV资源的做一些限制和规约及划分
  *pv和pvc存储类型和普通的nfs区别关系
    nfs就是一种目录共享的服务，对应“客户端pod”需要配置nfs类型的存储。 而CIFS、ClusterFS等也是目录共享服务，此时客户端如果用这些就要做不同类型的配置。
    为了让“客户端pod”开发者不用了解那么多的配置，PV和PVC就屏蔽了这些类型的目录共享服务。PV和PVC作为一种资源而存在，“客户端pod”只需要统一的配置即可。
    注:当pv1和pvc1绑定了，则kubectl get pv 命令执行后可以看见pv1的claim列值是pvc1，即代表了她与pvc1绑定。 相反的kubectl get pvc执行后可以看见pvc1的volume列值是pv1。
  *configmap类型的存储支持动态更新，但是更新需要一定的时间。 
 

*认证方式:
 k8s支持3种认证方式，可以同时配置，客户端只要通过其中一种认证，就可以相应的通信。  

*k8s集群配置文件默认在/.kube/config，通过此配置信息，Jenkins等持续集成应用可以连接到k8s集群
 当rancher创建的k8s集群的master主机上的kubectl无法正常使用时，可以到rancher具体集群上复制kubeconfig文件内容保存到主机的~/.kube/config （注意添加权限 chmod 600 /root/.kube/config）
 
？dockerfile和arges

？k8s集群工作节点添加已经会了，那么如何集群中的工作节点或主节点？

？部署了nginx后执行 kubectl get pods,service的CLUSTER-IP列是什么ip，有什么作用。！k8s集群内部ip，可以在集群的任何节点通过此cluster-ip访问该nginx服务
   而外部访问要用具体节点ip+PORT(S)列的端口能访问，如192.168.1.201:30741


？imagePullPolicy：IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地  本地没远程下载）
   那么这里的“本地”是指具体的k8s具体部署的节点，还是执行该yaml命令的节点

？kubectl get pods ... 和kubectl get pod ...的区别。 运行了kubectl get pods和kubectl get pods 似乎没有什么区别

？Deployment和ReplicaSet的区别

？kubectl run -it --image busybox busybox --rm /bin/sh
   该命令的作用是在 Kubernetes 集群中创建一个基于 BusyBox 镜像的临时 Pod，并启动一个交互式的终端会话。

？docker集群的高可用

？多个命令同时执行，可以用&&连起来？ kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev && kubectl rollout pause deployment pc-deployment  -n dev

？当k8s部署的一些应用动态的派发或者调整pod数量，这时候他们持久化的数据应该保存在哪里，又应该怎么去获取。 如redis的RDB备份文件。如mysql本地的idb数据文件
   Horizontal Pod Autoscaler(HPA)动态调整pod数量

？如果dockerpull一些镜像有问题，怎么通过浏览器下载完后再放到服务器上去使用
  参考黑马k8s教程天4文档“安装metrics-server”

？同时设置spec.selector.matchLabels和spec.selector.matchExpress，是叠加逻辑还是或逻辑
   ！这两个配置就是一起使用的，属于“基于集合的Label Selector”（label有两种类型：一种基于等式，一种基于集合）

*注意metrics使用kubectl top node命令是会报错，可以用kubectl top nodes

？需要跟视频学习
  ?天4  17~37（相关控制器概念）
  ?天4  130~132、169~181（怎么编辑？  ）   ！相当于vim打开配置文件，保存后自动更新
  ?天4  347~358 deploy更新策略？`重建更新`和`滚动更新`有啥区别及相关策略参数  ！更新策略指的是镜像更新策略。
        重建更新：全部老版本镜像删除后再重新启动全部新版本镜像。
        滚动更新：一小部分一部分老版本镜像删除更新，新旧版本镜像会同时存在。注意滚动更新时会同时存在老replicaSet和新replicaSet，但是老的replicaSet不会被删除（用于做版本回退）
  ?天4  347~469 版本回退整部分内容
  ?天4  575~764 HPA控制器实践过程，用什么压测？教程里用postman的runner压测。我用jmeter压测完，用kubectl get hpa -n dev -w查看，replicas升到10很久才降下来。
          ？ TARGETS列（如2%/3%）是什么意思！2%当前pod的cpu占用率，3%是阈值cpu占用率  ？targetCPUUtilizationPercentage：3意思是单个pod的cpu利用率达到3%就新开一个pod
  ?天4  1097~ Service介绍
  ?天2  apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead 整个初始化过程要听下 
  ?天2  pod重启策略，OnFailure ： 容器终止运行且退出码不为0时重启？  !即 异常退出

？ipvsadm 安装：https://blog.csdn.net/C3080844491/article/details/52783285

？docker本地打包好的镜像如何push到阿里云镜像源https://blog.csdn.net/Siebert_Angers/article/details/127439972  、  https://note.youdao.com/ynoteshare/index.html?id=f6d2a9e293f7817d99426841c5070b96&type=note&_time=1682478576703
  docker本地打包好的镜像如何上传到服务器上

？创建PV时，hostPath.path="/mnt/data"，那么这个目录在哪个节点上？ 
  比如mysql安装时使用PV进行挂载，则mysql对应的pod在哪个节点PV的实际hostPath.path目录就在pod所在节点上。可以用kubectl get pod -o wide查看具体节点。 但是如果多个pod呢？

？Rancher和k8s集群在同一局域网的不同网段，那么Rancher能管理k8s集群吗？！可以，但是网络联通传输会差些

？Rancher部署时遇到问题（环境准备用黑马k8s教程，rancher具体安装才看云原生）：
   在192.168.8.27上安装添加yum软件源时，导致yum不能使用，网络上很多方案都是改镜像源，清yum缓存等都无效。
   最后使用以下安装centos-release软件包方案也是没用，但是叠加加上“将docker-ce.repo文件暂时清除再用yum”就可以了。
   https://blog.51cto.com/u_14900374/2544835

*Rancher部署完后管理员账号 admin  密码 k8sadmin
  
*注意project和namespace的概念区别
 Project 是 Rancher 的组织单元，用于管理和控制多个集群和相关资源。
 Namespace 是 Kubernetes 的概念，用于在单个集群中进行资源隔离和管理。
 Project 可以包含多个集群和命名空间，提供集中管理和控制的功能。
 Namespace 用于在单个集群中隔离和管理不同的资源，并提供资源隔离和访问控制的功能。

*修改rancher的web样式，以规避监控
docker cp rancherContainerId:/usr/share/rancher/ui/assets /path/to/my-style/assets    复制原有的静态资源
docker cp rancherContainerId:/usr/share/rancher/ui-dashboard /path/to/my-style/ui-dashboard

docker stop rancherContainerId
docker run -d --privileged -p 80:80 -p 443:443 -v /opt/data/rancher_data:/var/lib/rancher  -v /path/to/my-style/assets:/usr/share/rancher/ui/assets  -v /path/to/my-style/ui-dashboard:/usr/share/rancher/ui-dashboard  --restart=always --name rancher-2.5.15 rancher/rancher:v2.5.15

？在ingress创建过程中，创建了（serviceName: nginx-service，servicePort: 80）和（serviceName: tomcat-service，servicePort: 8080）两个映射
   此处两个服务指定了不同的端口号，是为了避免两个个服务的pod实际部署到同一台主机节点上时端口冲突，但是如果要都使用80，是否可用反亲和性或者污点等功能处理？
 
*注意ingress资源使用时要注意：如果nginx-ingress-controller安装时会产生一个nginx应用（即ingress-nginx），如果默认端口不是80，则到时候创建ingress资源后，访问的域名后面要加端口号，如：tomcat.itheima.com:32240
 

？在rancher中界面创建ingress好像只能指定一个域名，然后通过不同路径区分？ 如果要通过子域名区分是不是只能通过rancher的yml配置？

*k8s部署zookeeper详见简书《K8s部署Zookeeper集群》   https://www.jianshu.com/p/f0b0fc3d192f   参考文档https://www.cnblogs.com/yxh168/p/14251188.html
 关注点：原本手工部署时每个zk都需要指定myid和在 zoo.cfg里配置所有zk节点的信息（详见淘淘doc7），那么在k8s里是怎么去配置这些信息呢？详见上述简书收藏，注意“ZOO_SERVER_ID” 对应myid

rancher创建k8s集群时设置私有镜像仓库
仓库地址：registry.cn-hangzhou.aliyuncs.com
用户名：845257580@qq.com
密码：g....

*rancher上用RKE创建k8s集群时有一个是否启用【Nginx Ingress】选项，如果启动则会在全部可部署负载应用的主机（一般是worker节点）上装上Ingress相关的nginx并占用80端口，如果不想这样可以禁用。


==========jeecg 服务编排 begin=============
*基本思路：
  1、用jeecg的docker-componse-base.yml进行所有应用镜像构建
       它会根据配置的资源目录进行构建docker镜像
  2、将上述构建的镜像push到自己的阿里镜像仓库
  3、将k8s集群里的docker镜像源修改成自己的阿里镜像仓库
  4、编写k8s相关yaml文件进行服务管理

*具体步骤：
 1、进jeecg-boot根目录，构建dev+SpringCloud环境下的jar包
 2、进jeecg-boot/jeecg-server-cloud分别执行docker-compose -f .\docker-compose-base.yml up -d和docker-compose up -d 构建运行测试镜像
 3、执行批处理文件push-images.bat，具体内容如下
 4、编写k8s相关yaml文件，具体内容如下：
 注：k8s相关资源配置详见jeecg-boot-k8s.yml




**********后端 push-images.bat begin****************
setlocal EnableDelayedExpansion
set mysqlVersion=7.0
set gatewayVersion=7.0
set systemVersion=7.0
set nacosVersion=7.0
set demoVersion=7.0

(
docker login --username=845257580@qq.com registry.cn-hangzhou.aliyuncs.com || exit /b

docker tag jeecg-server-cloud-jeecg-boot-mysql registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-mysql:!mysqlVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-mysql:!mysqlVersion! || exit /b

docker tag jeecg-server-cloud-jeecg-boot-gateway registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-gateway:!gatewayVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-gateway:!gatewayVersion! || exit /b

docker tag jeecg-server-cloud-jeecg-boot-system registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-system:!systemVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-system:!systemVersion! || exit /b

docker tag jeecg-server-cloud-jeecg-boot-nacos registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-nacos:!nacosVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-nacos:!nacosVersion! || exit /b

docker tag jeecg-server-cloud-jeecg-boot-demo registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-demo:!demoVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-demo:!demoVersion! || exit /b
)

endlocal

**********后端 push-images.bat    end****************


**********前端 push-images.bat    begin****************
setlocal EnableDelayedExpansion
set uiVersion=4.0

(
docker login --username=845257580@qq.com registry.cn-hangzhou.aliyuncs.com || exit /b

docker tag jeecgboot-ui registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-ui:!uiVersion! || exit /b
docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-ui:!uiVersion! || exit /b

)

endlocal

**********前端 push-images.bat    end****************



*编写pushImages.bat （写入以下内容），然后双击运行或者cmd里运行
 cmd /k "docker login --username=845257580@qq.com registry.cn-hangzhou.aliyuncs.com && docker tag jeecg-server-cloud-jeecg-boot-mysql registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-mysql:1.0 && docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-mysql:1.0"

?怎么在上传镜像到阿里云的命令里设置该仓库类型为公开
 chatGPT给的建议是加参数--disable-legacy-registry=false，如docker push registry.cn-hangzhou.aliyuncs.com/namespace-cny/jeecg-mysql:!mysqlVersion! --disable-legacy-registry=false 
 注意：测试无效可能是因为开了梯子，导致了网络访问出问题。

？kubectl apply -f xxx.yml执行后（镜像未变动，但pod配置有变动），涉及的pod并未重启

？在jeecg的application.yml等配置文件中，有用到环境变量和host域名的，在pod的spec下都可以进行相关的配置（env和hostAliases）

？redis部署后，理论在集群节点中任意一个节点都能访问，但是发现在system和gateway的pod里192.168.8.30:6379不能正常访问。 
   ！后来将system和gateway里应用的yml里的redis配置都改成192.168.8.31就都可以访问了 （注：要先启动system初始化路由再启动gateway）
  
==========jeecg 服务编排 end  =============

==========jeecg 服务Rancher编排 begin=============
*基本思路：
 0、自定义rancher主题（防偷窥）、将原有的k8s集群再扩展，否则资源可能不够用
 1、通过Rancher部署mysql、redis、es、kafka、jeecg-system集群
 2、通过Ranche将jeecg其它涉及中间件和服务部署为单机，并保证高可用
 3、对1中相关集群进行高可用，及并发性能测试。 es进行简历或公司数据处理搜索

*思考问题：
 ？mysql集群使用什么方案（主要测试应该是分库分表）、，是否要测试canal中间件
    RadonDB MySQL Kubernetes是一种基于Kubernetes的分布式MySQL解决方案

*具体步骤：
   0、扩展k8s集群192.168.8.24/25/28/29/30/31 其中28master，另外27位Rancher服务
   1、mysql基于原本的PV-PVC-HostPath部署两个分库、redis基于redis-cluster部署、es和kafka再学习部署、jeecg-system集群做hpa部署
   2、其它中间件及服务随便部署

? 通过rancher ui或rancher dashboard ui创建deployment时，会根据【网络模式】创建对应的service。
  如【网络模式】= NodePort时则会创建对应的NodePort类型Service，而且还会创建一个CLusterIP类型的Service。所以它自动帮创建了2个service用于暴露服务。注意：用rancherui会比rancher dashboard创建更容易

？发现通过kubeadm创建的k8s集群，再导入到rancher中，则在rancher中找不到可以添加新的主机节点的按钮。 
  ！此时可以通过kubeadm join进行添加

？假如k8s主机有3台，但是一个微服务集群有4个，那么如何去部署？

==========jeecg 服务Rancher编排 begin=============

========192.168.1.28 begin========
TYPE="Ethernet"
PROXY_METHOD="none"
BROWSER_ONLY="no"
BOOTPROTO="none"
DEFROUTE="yes"
IPV4_FAILURE_FATAL="no"
IPV6INIT="yes"
IPV6_AUTOCONF="yes"
IPV6_DEFROUTE="yes"
IPV6_FAILURE_FATAL="no"
IPV6_ADDR_GEN_MODE="stable-privacy"
NAME="eth0"
UUID="6749d7c2-a0cc-4fb9-a39f-eba1be7245fc"
DEVICE="eth0"
ONBOOT="yes"



192.168.8.29    cdh-1
192.168.8.24    cdh-2
192.168.8.25    cdh-3
192.168.8.26    cdh-4
192.168.8.27    cdh-5
192.168.8.28    cdh-6
192.168.8.30    cdh-7
192.168.8.31    cdh-8
192.168.8.32    cdh-9
124.71.27.239   cdh1
122.9.49.145    cdh2
122.9.126.41    cdh3
122.9.122.146   cdh4
122.9.53.112    cdh5
116.63.74.203   cdh6

*集群节点：
192.168.8.28    cdh-6
192.168.8.30    cdh-7
192.168.8.31    cdh-8
 使用kubeadm init创建集群后得到的响应（包含主节点token）如下：
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.8.28:6443 --token hwk5nc.fcip9c4al5uwc3x6 \
    --discovery-token-ca-cert-hash sha256:7a7ff9adee3dd020c158e1ed3b94533c3ad32a96b7f53b40e075a8c02232803c

========192.168.1.28 end========

？我要禁用swap分区并重启linux，会不会有影响

# 重新加载配置
[root@master ~]# sysctl -p
******
[root@cdh-7 ~]# sysctl -p
vm.swappiness = 0
[root@cdh-7 ~]#
******
[root@cdh-7 ~]# lsmod | grep br_netfilter
br_netfilter           22256  0
bridge                151336  1 br_netfilter
[root@cdh-7 ~]#


ETHTOOL_OPTS="autoneg on"
IPADDR="192.168.8.28"
PREFIX="24"
GATEWAY="192.168.8.1"
IPV6_PRIVACY="no"
DNS1="192.168.1.13"

注意：你创建集群时，选择的角色如果只有 Control Plane, Etcd 的话，会一直等待 worker 节点加入才会开始创建集群
================== rancher begin =======================
*rancher部署时，基础环境安装参考传智k8s教程资料文档（到k8s集群初始化部分之前即可）

*如果要多集群管理，Rancher版本要2.5+

*在dockerhub里搜索Rancher时一些版本名称比较长的，通常是一些不稳定之类的版本

*查看docker根目录，使用命令docker info可以看到Docker Root

*用Rancher直接可以部署k8s集群，但是好像没有高可用性。 要实现高可用，要安装rancher一些组件

*新版的k8s已经不支持docker了，而是containerD，实际上也是docker，只是换了个名字而已，只是不支持dockershim

*rancher版本2.5.2-linux-amd64
 k8s版本v1.19.15-rancher1-1
 默认网络驱动Canal（集成了flannel和calico）

*运行rancher容器时，要把数据挂载出来

？master都挂了 woker节点可以自动升级为master？


*Rancher中文文档：https://docs.rancher.cn/

*kube_config_rancher-cluster.yml ：集群的 kubeconfig 文件，它包含了访问集群的全部权限。如果 Rancher 出现故障，无法运行，您可以使用这个文件连接通过 Rancher 部署的 Kubernetes 集群（RKE 集群）。

*您可以在单个节点或高可用的 Kubernetes 集群上安装 Rancher。由于单节点安装只适用于开发和测试环境，而且单节点和高可用集群之间无法进行数据迁移，所以我们建议您从一开始就使用高可用的 Kubernetes 集群来部署 Rancher Server，而且您需要分开部署运行 Rancher Server 的集群和运行自己业务的下游集群。

*上游集群：指的是部署rancher server的集群，实现rancher的高可用和etcd数据持久化。对于这个集群建议每个节点都有所有角色，这样比较安全
 下游集群：指定就是我们业务应用的集群，受rancher控制。对于业务应用的集群，建议为每个节点使用单独的角色，这可以保证工作节点上的工作负载不会干扰 Kubernetes Master 或集群数据。

*RKE等相关名词解释详见https://docs.rancher.cn/docs/rancher2/overview/glossary/_index  【名词解释】

*针对 Rancher 2.4 的高可用部署，也将提供另外一种通过 K3s 集群安装 Rancher HA 的方法，从而大大简化部署 Rancher 高可用的流程。

*流水线：使用 Rancher，您可以与 GitHub 等版本控制系统集成，以设置持续集成（CI）流水线。
 配置 Rancher 和 GitHub 等版本控制系统后，Rancher 将部署运行 Jenkins 的容器以自动化执行流水线：
	构建镜像
	验证镜像
	部署镜像到集群
	执行单元测试
	执行回归测试

*Rancher v2.3.0 可以使用 Windows Worker 节点，Windows Server 的节点必须使用 Docker 企业版。这是 Kubernetes 的限制。
 Windows 节点只能用于工作节点。详情请参阅配置自定义 Windows 集群。

*硬件要求：
  具有worker角色的节点的硬件要求主要取决于您的工作负载。运行 Kubernetes 节点组件的最小值是 1 个 CPU（核心）和 1GB 内存。
  关于 CPU 和内存，建议将不同平面的 Kubernetes 集群组件（etcd、controlplane 和 worker）托管在不同的节点上，以便它们可以彼此分开扩展。


*配置 RKE 集群模板时，管理员具有以下权限：
    决定用户可以修改的和不可修改的集群选项。
    指定的用户是否可以与其他用户或和用户组共享 RKE 集群模板。
    为不同的用户组创建 RKE 集群模板。
*集群和模板之间存在多对一的关系，用户可以使用一个模板创建多个集群，而每个集群有且只有一个对应的模板。确认使用的模板后，无法修改。
 例如，“集群 A”是用户使用“模板 a”创建的，那么在编辑集群时，用户不能将集群 A”对应的“模板 a”，修  改为其他模板。管理员可以通过编辑集群模板的方式更新集群模板，应用该模板创建的集群就会自动适配新模板的参数。

*rancher中k8s集群的节点列表可以看到etcd和Control(即master)节点上有“node-role.kubernetes.io/etcd=true:NoExecute”和“node-role.kubernetes.io/controlplane=true:NoSchedule”污点标记

*rancher中k8s集群的节点新增control节点时，会更新其它的work节点，此时work节点的状态并不是active

*rancher部署k8s集群时最好提前规划好etcd、control、work节点的分布。否则后续更改角色，特别是多角色删减容易出问题，因为各种角色存储的状态信息在增删时都会有所同步。
官方建议每种角色都分配在不同的节点上。  为了充分利用资源，可以将etcd和control分配到同一个节点上，因为这两个节点是不派发pod的，属于“运维节点”

*rancher创建的k8s集群出错不可用时，不能进入集群部署应用。此时可以通过rancher里的集群“恢复”进行备份恢复（创建集群会自动备份，有需要可以自己备份）。
 可能也会导致部署的应用不能正常访问

*k8s基础环境搭建时会启动一些docker容器，如kube-apiserver、kube-controller-manager、kube-scheduler、kube-proxy、pause、etcd、coredns以及（kubeadm、kubelet和kubectl）
 在k8s集群节点出问题时，停止删除掉上述容器之外的rancher-agent镜像删除既可

*rancher界面顶部有【命令行】，用于执行kubectl相关集群命令，原理：它不是连接到指定master主机而是将输入的kubectl命令发送至多个master主机中执行并将返回的结果显示出来
 rancher RKE部署的k8s集群
 注意：在 Rancher 的命令行界面中helm基本上是不可用的，只能到应用商店中去操作。 或者在rancher所在的主机上安装helm工具，在主机上执行命令。

*rancher里怎么用helm部署应用：要到具体项目的命名空间中去，然后点击【应用商店】-【启动】
 除了helm以外，还可以添加其它的应用
 

？定期执行下游集群备份怎么做？Kubernetes 使用 etcd 来存储其所有数据--从配置、状态和元数据。在灾难恢复的情况下，备份这些数据是至关重要的。
？rancher中文网的《 Kubernetes 的 101 种安全最佳实践》文章不能访问，网上找下

?rancher创建k8s集群时，设置了私有镜像仓库，会导致出现节点创建各种失败（通过到节点主机上执行dockers logs进行查看错误日志）
 这个问题搞了好久，最终才发现是设置了阿里云私有镜像仓库

？rancher上的命令行是在环境下（主机？目录？）

？用rancher启动一个普通测试的nginx工作负载，一直访问不到9090
  对于nginx镜像，默认端口是80，所以容器默认端口要80，我直接设置容器端口和外部端口是9090。 
  所以一个镜像监控什么端口，容器就设置什么端口，一般不变，除非启动容器时设置相关的端口参数进行变更

？rancher创建k8s集群后并且部署了应用（如nginx），要扩展k8s集群主机节点，但是找不到添加入口。
   可以编辑k8s集群（随便改下名称），然后完成后就可以看到添加节点按钮了。


？rancher上怎么让一个拥有control和worker和etcd角色的主机节点变成只有work角色？
  在 Rancher 管理界面中，没有直接提供编辑节点角色的功能。在 Rancher 管理界面中，没有直接提供编辑节点角色的功能。可以按以下操作：
  1、确保您有一个备份节点的数据的计划，以防需要还原到之前的状态。
  2、从 Rancher 管理界面中移除该节点，即将其从集群中删除。
  3、在该节点上，重置 Kubernetes 组件的配置。这将涉及删除现有的 Kubernetes 组件，并重新初始化该节点以仅担任 Worker 角色。
       您可以使用适当的工具（如 kubeadm）来执行此操作。确保在进行这一步之前，已备份所需的数据和配置。
       #删除容器运行时的相关容器
       sudo docker rm -f $(sudo docker ps -aq)
       sudo systemctl stop kubelet
       sudo systemctl stop docker  如果停止不了可以再执行sudo systemctl stop docker.socket和sudo systemctl stop docker.service，让其完全停止
       #删除 Kubernetes 组件的数据和配置文件。请确保在执行之前备份重要的数据。
       sudo rm -rf /etc/kubernetes   
       sudo rm -rf /var/lib/kubelet   如果删不掉，可用df -HT | grep '/var/lib/kubelet/pods'查看挂载信息，然后umount $(df -HT | grep '/var/lib/kubelet/pods' | awk '{print $7}') 就可以删除了
       sudo rm -rf /var/lib/etcd
       
       #清除容器网络配置。
       sudo ip link del cni0
       sudo ip link del flannel.1
       
  4、在 Rancher 管理界面中重新添加该节点，并将其作为 Worker 节点加入到集群中。这样，该节点将以单个 Worker 角色重新加入集群。
    特别注意：涉及k8s集群节点操作尽量走一步看一步，每个操作都要等整个集群状态正常后（active），再进行其它操作。 如添加注册节点也不要同时进行，一个个添加。

 ================== rancher end =======================

  To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.8.28:6443 --token 4vulew.kkongvbxsvokyl1a \
    --discovery-token-ca-cert-hash sha256:bc7f7c2bb5aeaea5c30fd181c00ba3c3e3fc7a950f95028c25f8bf2d9b5d7751

=========将k8s及docker等版本更换为别的版本 begin===============
*如以下具体版本
Rancher Version: 2.5.17
Kubernetes Version: 1.20.15
Docker version: 3:20.10.9-3.el7   yum安装docker时3:是epoch不需要加
pause:3.2
etcd:3.4.13-0
coredns:1.7.0


*在安装pause和etcd和coredns时，不知道和k8s相兼容版本怎么办？ 可以用kubeadm config images list


docker run -d --privileged -p 80:80 -p 443:443 -v /opt/data/rancher_data:/var/lib/rancher --restart=always --name rancher-2.5.17 rancher/rancher:v2.5.17
docker run -d --privileged -p 80:80 -p 443:443 -v /opt/data/rancher_data:/var/lib/rancher --restart=always --name rancher-2.5.17 rancher/rancher:v2.5.17 --entrypoint /usr/bin/entrypoint.sh

yum install --setopt=obsoletes=0 kubeadm-1.20.15 kubelet-1.20.15 kubectl-1.20.15 -y

images=(
    kube-apiserver:v1.20.15
    kube-controller-manager:v1.20.15
    kube-scheduler:v1.20.15
    kube-proxy:v1.20.15
    pause:3.2
    etcd:3.4.13-0
    coredns:1.7.0
)
for imageName in ${images[@]} ; do
	docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
	docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName 		k8s.gcr.io/$imageName
	docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
=========将k8s及docker等版本更换为别的版本   end===============

？linux的crontab命令使用

*当主机节点被docker容器应用占用时，用netstat -anp | grep 80查看到的pid和应用名是这样的“10709/docker-proxy”

cdh-1 80被占用
cdh-7 80可用
cdh-8 80要用
？一般类似kafka等集群部署后怎么去访问
？创建k8s集群时ingress没有初始化是否会影响kafka集群的暴露

？rancher里用helm安装kafka，操作配置界面上为什么更改不了kafka版本？
  在Rancher中使用Helm安装Kafka时，操作配置界面上可能无法更改Kafka版本是因为该Helm Chart固定了Kafka版本，并在配置界面上禁用了版本更改选项。这是由于Helm Chart的作者在创建Chart时选择了特定的Kafka版本，并将其固定在Chart的配置中。  可以自定义helm或者直接用helm命令工具（？在rancher上的命令行界面能否操作？）
  
 ？在redis或kafka等集群下，通常都是一个Headliness类型service对应多个应用pod，而service负责暴露应用给k8s集群内部其它应用连接，但却无法暴露到k8s集群外部或外网。 
    那么实际上开发者在开发电脑上做本地程序时，是如何连接到这些集群的具体pod节点，因为通常连接客户端自己做重定向或负载均衡。

？遇到启动rancher时，docker logs rancherId查看时报“exec /usr/bin/entrypoint.sh: no such file or directory”相关错误

https://github.com/helm/charts/tree/master/stable/kafka-manager


aliy服务器搭建docker环境前后资源对比（在阿里云后台查看）
之前：
 CPU使用率 1.849% 
 内存使用率 14.79%
 云盘使用率 76%
之后：
 CPU使用率 41.692%
 内存使用率 45.53%
 云盘使用率 65%
安装elk后：
 CPU使用率 99.618%
 内存使用率 60.92%
 云盘使用率 66%


**************************company 搜索引擎搭建 begin******************************************

*环境组件
 k8s环境（rancher、docker）、kafka
 Flink单机集群模式搭建（断点续传）、Mysql（开启binlog）
 elk、Appsearch单机
 应用类jeecg、

*资源
 2核8G 阿里ECS  2025/2/2到期
 2核4G 华为云HECS  2024/07/17到期

*服务器购置：
 网友经验：https://zhuanlan.zhihu.com/p/643910185
 ？需要另外装显卡吗
 ？可以帮装系统吗？如果自己买硬盘怎么装、怎么装系统
 ？保修多久？ 有问题退回包邮么？
 ？如果加装固态硬盘，会不会有什么问题？ 
？这个配置内存是什么内存？ 内存  32g 三星 2133的  
？ddr4的吧  是
？730xd  2686v4*2  16G*24条  h730p卡 单750w电源 12个空盘架 这样配置要多少钱？ 3400元
   730 3.5寸 8盘位 2686v4*2  16G*24条  h730卡 单750w电源 8个空盘架   3750元
   730 3.5寸 8盘位 2686v4*2  16G*8条  h730卡 单750w电源 8个空盘架   2459元
   单个硬盘多少钱？  单个东芝 6t sas 12gb  sas 企业级的价格 275元、  希捷 2t sas  0023  89元 、希捷 3t sas 650ss  119元、inlte  3520 960g 245元一个
？有没有全新的硬盘帮安装调试再发
？盘位3.5还是2.5
？配一台服务器，质保多久  1年
？尺寸呢？戴尔730服务区尺寸 ：682*434*68 长*宽*高
？730可以接正常的显示屏吧？ 730XD好像是VGA口？ 不能用转换器，只能买独显。 原装丽台K2200显卡335元
？哪一年的服务器？ 一般16年的机器，不低于这个年份
？有问题怎么处理？ 退换货包邮么？7天内可以退、1年内可以换

网友经验：https://www.bilibili.com/read/cv24558826/
PS：
（1）短显卡安装在提升卡2位置需要注意，供电口会被提升卡3挡住，两者只能任选一个，建议显卡上长显卡，或者是不需要供电的显卡。
（2）R730支持最多四个单宽显卡或最多两个双宽显卡。而R73XD则支持最多三个单宽显卡或最多一个双宽显卡，当然，这些都是常规模式安装，外挂另当别论。 
（3）所有槽位都是X16的连接器，无需担心X8的规格是否可以插上 

*重要的事情说三遍：你不主动询问，某些店家不会给你说，内存是多少频率的，是1R还是2R的；硬盘是SAS还是SATA的，转速是多少；电源是不是EPP的；网卡是博通的还是英特尔的；是电口还是光口；散热风扇是不是水洗过。

*适配这款机器的风扇有好几家供应商，有AVC、SAN ACE60(三洋）、DELTA(台达）、SUNON(建准) 、NMB（美蓓亚），我自己买到的是NMB,是比较静音的

*"刷直通"是指在二手服务器中刷入直通（Directly Attached Storage，简称DAS）固件。刷直通的过程是将服务器的本地存储（如硬盘或SSD）直接连接到服务器主板上的SAS或SATA接口，而不是通过RAID控制器进行连接。

*如果忘记了iDRAC的账号密码，可以尝试以下两种方式重置：
 通过BIOS重置。开机时按F2进入BIOS，选择DRAC settings。在user configuration界面下，可更改密码。选择enabled，在change password中输出新密码，然后点击Back后再次输入密码。修改完之后重启服务器，用新密码就能登录idrac界面了。
 通过console控制台重置。联系系统组同事协助操作，关机后按F12进入ESXi界面，输入ESXi主机用户名和密码，重启服务器。按提示按F10进入到IDRAC设置项，在系统设置处输入新密码并再次确认，保存设置后重启服务器。等待服务器启动到ESXi正常界面，用新密码就能正常访问和登陆IDRAC了。
 

**************************company 搜索引擎搭建    end******************************************

********************************戴尔服务器 begin*****************************************
*虚拟化方案研究：
 DELL实体服务器安装ESXI进行虚拟化        
    https://blog.csdn.net/linmengmeng_1314/article/details/113973058?ops_request_misc=&request_id=&biz_id=102&utm_term=%E6%88%B4%E5%B0%94%E6%9C%8D%E5%8A%A1%E5%99%A8%20%E8%99%9A%E6%8B%9F%E5%8C%96&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-113973058.142^v94^insert_down1&spm=1018.2226.3001.4187
 Dell 服务器之开启虚拟化功能
   https://blog.csdn.net/weixin_57798411/article/details/127529930?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169500765316800226560252%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169500765316800226560252&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-127529930-null-null.142^v94^insert_down1&utm_term=%E6%88%B4%E5%B0%94%E6%9C%8D%E5%8A%A1%E5%99%A8%20%E8%99%9A%E6%8B%9F%E5%8C%96&spm=1018.2226.3001.4187
 Dell R720 iDRAC +Vmware Esxi5.1 安装
   https://blog.csdn.net/weixin_34378767/article/details/92859953?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522169500657916800184168476%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=169500657916800184168476&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-92859953-null-null.142^v94^insert_down1&utm_term=idrac%20vmware&spm=1018.2226.3001.4187
 bilibili上的《花小钱办大事，打造一台虚空电脑——实战服务器虚拟化》 05:01选择SD卡启动、使用的也是esxi
    ESXi可以被理解为一种基于虚拟化技术的操作系统。它可以直接安装在物理服务器上，无需其他操作系统，是一种轻量级的操作系统。
    vCenter Server和ESXi是vSphere环境中的两个重要组件，vCenter Server则是一个管理工具，提供了一个集中的控制台，用于管理和监控一个或多个ESXi服务器。

*手把手教会你：VMware Esxi系统安装步骤（版本7.0.3）   https://blog.csdn.net/weixin_62958056/article/details/130219901
     https://blog.csdn.net/Sinck7/article/details/132424956
 Dell PowerEdge R630服务器VMware ESXI6.0服务器安装    https://blog.csdn.net/anfeng3697/article/details/101565921
*vmware workstation和vmware esxi有什么区别？
 workstation是运行在操作系统里的，算是一个软件。一般用于个人PC进行虚拟机创建
 esxi是直接运行在物理机硬件上的，不需要先装一个操作系统（本身就算是一个带虚拟化的操作系统），一般用于服务器虚拟机的创建。

*iDRAC
 详见bilibli教程
  https://www.bilibili.com/video/BV1rf4y187N6/?spm_id_from=333.788&vd_source=5d74800eec4782c9be5ee53ff7c9a201
  https://www.bilibili.com/video/BV1uk4y1M7vg/?spm_id_from=333.337.search-card.all.click&vd_source=5d74800eec4782c9be5ee53ff7c9a201
 *两种连接方式：
   方式一：前面板mic口接笔记本USB接口，浏览器输入默认ip 159.2540.3 
   方式二：背面板网卡口接入到和笔记本同个局域网，然后浏览器输入ip（从开机启动界面获取）
 *创建多少虚拟磁盘合适？
 一般来说，建议为每个虚拟机创建一个独立的虚拟磁盘，以获得更好的性能和可靠性。但是具体的数量需要根据实际情况进行权衡和决策。
 因为一个物理磁盘划分出多个虚拟磁盘后必然带来性能的消耗，磁盘读写速度也会削弱，所以如果对于磁盘性能要求不高可以多分几个。
 而多个虚拟机公用一个虚拟磁盘，也必然带来磁盘读写分配的区分计算等消耗，所以如果对于磁盘性能要求不高可以多个虚拟机公用一个虚拟磁盘


*VMware ESXI下载教程 https://blog.csdn.net/Sinck7/article/details/132424956
 下载地址直接进  https://customerconnect.vmware.com/cn/downloads/#all_products
  VMware网站账号密码 3331866906@qq.com   Gdlr54288,,
  最后到下载页面，下载按钮disabled，应该是个人用户无权下载。
********************************戴尔服务器 end*****************************************

*阿里云2核8G，安装centos7，部署了铁哒基础应用内存占用过大（mysql8、redis3、nginx），装完基础应用仅剩不到1G内存，导致java应用无法启动。
 一开始都是yum安装，并且系统是centos7。后面换AlibabaCloudLinux2并且安装了mysql5.7，内存就大幅度下降，，装完基础应用还可以有7G内存。